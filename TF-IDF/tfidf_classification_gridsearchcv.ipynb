{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brush\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search is all the way at the bottom, load the data up here then I think you can just skip to the bottom mega brick of code that has grid search and run that and it works.\n",
    "  \n",
    "  \n",
    "  Currently using multinomial Naive Bayes as based on my preliminary research it seemed best for what we were doing however I could potentially be wrong about that so we could always switch it to something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ #normalized data, can be done without normalized data but threshold will need to be changed\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regions(image, size): #divide image into non overlapping regions of specified size\n",
    "    image.numpy()\n",
    "    regions = []\n",
    "    for i in range(0, 28, size): \n",
    "        for j in range(0, 28, size):\n",
    "            region = image[0, i:i+size, j:j+size]\n",
    "            regions.append(region)\n",
    "    return regions\n",
    "\n",
    "def encode(region, threshold):\n",
    "    region = torch.where(region < threshold, 0, 1)  # Vectorized thresholding\n",
    "    binary_str = ''.join(map(str, region.flatten().int().tolist()))  # Convert to binary\n",
    "    return int(binary_str, 2)  # Convert binary to decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_size = 4 # for 4x4 region max binary value is 65535\n",
    "threshold = -0.5\n",
    "\n",
    "encoded_images = []\n",
    "for image, label in train_dataset: #with the for loops, took my laptop around 30 min to run on 4x4\n",
    "    r = regions(image, region_size)\n",
    "    for i in range(len(r)):\n",
    "        r[i] = encode(r[i], threshold)\n",
    "    encoded_images.append(r) #in theory this encodes all the regions and stores them. each image is its own list of decimal regions\n",
    "    \n",
    "for i in range(len(encoded_images)):\n",
    "    encoded_images[i] = [x for x in encoded_images[i] if x != 0] #remove any regions that are just 0 (all black/no important features according to threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTH:  60000\n",
      "LENGTH:  10\n"
     ]
    }
   ],
   "source": [
    "text_data = [' '.join(map(str,encoded_images[i])) for i in range(len(encoded_images))] #joins the regions together into one string instead of a list of strings\n",
    "\n",
    "df = {'Document':text_data, 'Label':train_dataset.targets.tolist()} #dataframe of images and labels\n",
    "df = pd.DataFrame(df)\n",
    "print('LENGTH: ', len(df))\n",
    "\n",
    "groupeddf = df.groupby('Label', as_index=False).agg({'Document': ''.join}) #dataframe of documents grouped by class (all documents of one class are combined into a mega document)\n",
    "print('LENGTH: ', len(groupeddf))\n",
    "nums = groupeddf['Document']\n",
    "labels = groupeddf['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>311 61439 12 4407 65500 64896 52430 19 64716 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>887 2184 17 65518 14335 51200 4403 61132 30719...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>63 1791 14194 65216 13175 3 2047 32751 311 652...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>55 2047 4095 2252 28672 61440 62335 52424 887 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>76 3276 1 52428 273 52360 4369 52431 31 255 16...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                           Document\n",
       "0      0  311 61439 12 4407 65500 64896 52430 19 64716 6...\n",
       "1      1  887 2184 17 65518 14335 51200 4403 61132 30719...\n",
       "2      2  63 1791 14194 65216 13175 3 2047 32751 311 652...\n",
       "3      3  55 2047 4095 2252 28672 61440 62335 52424 887 ...\n",
       "4      4  76 3276 1 52428 273 52360 4369 52431 31 255 16..."
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupeddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for Class 0:\n",
      "['65535', '32768', '30583', '34952', '65534', '52428', '13107', '4369', '4095', '32767']\n",
      "[0.054236074980597265, 0.03819115522705638, 0.0371329964032447, 0.03636803701866773, 0.03367587900251205, 0.032597379521376556, 0.031827154556204384, 0.031385897211588146, 0.030283610631406856, 0.03019481598182678]\n",
      "Top words for Class 1:\n",
      "['30583', '34952', '13107', '34816', '65535', '32768', '13175', '4403', '61132', '17']\n",
      "[0.13020339861054825, 0.09340019731559018, 0.06718764045033856, 0.05533996791573236, 0.05267887757797038, 0.048416603798303176, 0.043771310575540935, 0.04353944867440237, 0.043515719627089125, 0.042677337087875154]\n",
      "Top words for Class 2:\n",
      "['65535', '65520', '15', '65280', '32768', '4096', '12', '32767', '255', '61440']\n",
      "[0.06041848293700579, 0.04309225396078819, 0.038523567441857344, 0.036908608170267095, 0.03130764882374524, 0.03115215974522641, 0.026969546389008423, 0.026936917223232507, 0.026153947507166084, 0.024963010483831014]\n",
      "Top words for Class 3:\n",
      "['4095', '61440', '4096', '255', '65535', '65520', '12288', '28672', '136', '140']\n",
      "[0.059730799720494526, 0.046797132752346336, 0.0437398855686895, 0.04054819195120148, 0.039617726877021746, 0.03431834002206793, 0.03376137948721864, 0.029797748880999123, 0.027460917656893614, 0.025991817147242205]\n",
      "Top words for Class 4:\n",
      "['32768', '52428', '4096', '4369', '17', '34952', '34816', '13107', '65535', '61166']\n",
      "[0.04179098851600414, 0.0412518051433898, 0.04083636106829522, 0.03881819064381895, 0.03594948945895628, 0.035263525690643595, 0.03290961842536449, 0.03277390346814763, 0.03214965411985951, 0.03161066182478472]\n",
      "Top words for Class 5:\n",
      "['255', '61440', '4095', '65280', '4096', '15', '65520', '32768', '49152', '12288']\n",
      "[0.05586065325643322, 0.05578757358256088, 0.04801445260094732, 0.045676532664797743, 0.03845353338317118, 0.03597156726354176, 0.03419838642352386, 0.0292341430018016, 0.028280803759154465, 0.028096581701809516]\n",
      "Top words for Class 6:\n",
      "['65280', '65535', '32768', '17', '19', '34816', '13107', '136', '51200', '4096']\n",
      "[0.061433981864467226, 0.045583360197298625, 0.0388694275897364, 0.03319481529686791, 0.03138070425794789, 0.030987432199116267, 0.030143144620908363, 0.028371720327702916, 0.028209050034902327, 0.027920652917386917]\n",
      "Top words for Class 7:\n",
      "['15', '32768', '65535', '65280', '34816', '17', '12', '65520', '14', '255']\n",
      "[0.08346517768539827, 0.05057009609780717, 0.04019750523693875, 0.038692065623780025, 0.038046151572013964, 0.03786096473626194, 0.035129234999569575, 0.03439294718445713, 0.03317922687117662, 0.030660094713602162]\n",
      "Top words for Class 8:\n",
      "['65535', '4095', '4096', '32768', '61440', '12288', '4369', '13107', '51200', '28672']\n",
      "[0.04490129986247859, 0.038424775691731264, 0.037037658323283845, 0.03403842388510519, 0.030380975944590168, 0.026323773074682012, 0.025760733035082733, 0.02438213981434596, 0.023250984099588666, 0.023039505393619323]\n",
      "Top words for Class 9:\n",
      "['255', '52428', '32768', '4352', '4096', '61166', '34952', '4369', '34816', '15']\n",
      "[0.04961407262400314, 0.041220698321456306, 0.03630360882260889, 0.033691488305356364, 0.033416764657706495, 0.030977496194304455, 0.029909006943955103, 0.028924800547638803, 0.027745769074824914, 0.02698707526959494]\n"
     ]
    }
   ],
   "source": [
    "#cvectorizer = CountVectorizer()\n",
    "#count = cvectorizer.fit_transform(features)\n",
    "#nums = cvectorizer.get_feature_names_out()\n",
    "ctfidf, features = BERTopic()._c_tf_idf(groupeddf, fit=True) #class based TFIDF\n",
    "ctfidf_array = ctfidf.toarray()\n",
    "\n",
    "score_threshold = 0.03\n",
    "\n",
    "topdata = []\n",
    "topweights = []\n",
    "scaled_top_words = []\n",
    "for idx, topic in enumerate(groupeddf['Label']): #print the top 10 words for each class\n",
    "    print(f\"Top words for Class {topic}:\")\n",
    "    top_indices = [i for i in range(len(features)) if ctfidf_array[idx][i] > score_threshold]\n",
    "    top_words = [features[i] for i in top_indices]\n",
    "    top_weights = [ctfidf_array[idx][i] for i in top_indices]\n",
    "    topdata.append(top_words)\n",
    "    topweights.append(top_weights)\n",
    "    print(top_words)\n",
    "    print(top_weights)\n",
    "    \n",
    "    expanded_words = []\n",
    "    for word, weight in zip(top_words, top_weights):\n",
    "        count = round(weight * 1000)\n",
    "        expanded_words.extend([word] * count)\n",
    "\n",
    "    # Append expanded words list for this class\n",
    "    scaled_top_words.append(expanded_words)\n",
    "    \n",
    "#For whatever reason it pulls in the same number as the highest rated number for all classes\n",
    "#Can we just scrap those or is it a broader issue and are the rest of the numbers wrong?\n",
    "#Same thing happened when the model was broken and was only 1s, it had all 1s preceded by the class number\n",
    "#Here it is all 0s preceded by the class number\n",
    "#also maybe need to go even bigger on the region based on the fact that the highest rated number for 3 classes is a white brick of all 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top weights for Class 0:\n",
      "[array([18398,  5907,  4856,  7135, 18345, 11305,   777,  8710,  8490,\n",
      "        5901], dtype=int64)]\n",
      "Top weights for Class 1:\n",
      "[array([18398,  5907,  4856,  7135, 18345, 11305,   777,  8710,  8490,\n",
      "        5901], dtype=int64), array([ 4856,  7135,   777,  6684, 18398,  5907,   881,  8737, 14607,\n",
      "        1773], dtype=int64)]\n",
      "Top weights for Class 2:\n",
      "[array([18398,  5907,  4856,  7135, 18345, 11305,   777,  8710,  8490,\n",
      "        5901], dtype=int64), array([ 4856,  7135,   777,  6684, 18398,  5907,   881,  8737, 14607,\n",
      "        1773], dtype=int64), array([18398, 18232,  1368, 17626,  5907,  8505,   126,  5901,  2939,\n",
      "       15359], dtype=int64)]\n",
      "Top weights for Class 3:\n",
      "[array([18398,  5907,  4856,  7135, 18345, 11305,   777,  8710,  8490,\n",
      "        5901], dtype=int64), array([ 4856,  7135,   777,  6684, 18398,  5907,   881,  8737, 14607,\n",
      "        1773], dtype=int64), array([18398, 18232,  1368, 17626,  5907,  8505,   126,  5901,  2939,\n",
      "       15359], dtype=int64), array([ 8490, 15359,  8505,  2939, 18398, 18232,   176,  3846,   954,\n",
      "        1066], dtype=int64)]\n",
      "Top weights for Class 4:\n",
      "[array([18398,  5907,  4856,  7135, 18345, 11305,   777,  8710,  8490,\n",
      "        5901], dtype=int64), array([ 4856,  7135,   777,  6684, 18398,  5907,   881,  8737, 14607,\n",
      "        1773], dtype=int64), array([18398, 18232,  1368, 17626,  5907,  8505,   126,  5901,  2939,\n",
      "       15359], dtype=int64), array([ 8490, 15359,  8505,  2939, 18398, 18232,   176,  3846,   954,\n",
      "        1066], dtype=int64), array([ 5907, 11305,  8505,  8710,  1773,  7135,  6684,   777, 18398,\n",
      "       14921], dtype=int64)]\n",
      "Top weights for Class 5:\n",
      "[array([18398,  5907,  4856,  7135, 18345, 11305,   777,  8710,  8490,\n",
      "        5901], dtype=int64), array([ 4856,  7135,   777,  6684, 18398,  5907,   881,  8737, 14607,\n",
      "        1773], dtype=int64), array([18398, 18232,  1368, 17626,  5907,  8505,   126,  5901,  2939,\n",
      "       15359], dtype=int64), array([ 8490, 15359,  8505,  2939, 18398, 18232,   176,  3846,   954,\n",
      "        1066], dtype=int64), array([ 5907, 11305,  8505,  8710,  1773,  7135,  6684,   777, 18398,\n",
      "       14921], dtype=int64), array([ 2939, 15359,  8490, 17626,  8505,  1368, 18232,  5907,  9201,\n",
      "         176], dtype=int64)]\n",
      "Top weights for Class 6:\n",
      "[array([18398,  5907,  4856,  7135, 18345, 11305,   777,  8710,  8490,\n",
      "        5901], dtype=int64), array([ 4856,  7135,   777,  6684, 18398,  5907,   881,  8737, 14607,\n",
      "        1773], dtype=int64), array([18398, 18232,  1368, 17626,  5907,  8505,   126,  5901,  2939,\n",
      "       15359], dtype=int64), array([ 8490, 15359,  8505,  2939, 18398, 18232,   176,  3846,   954,\n",
      "        1066], dtype=int64), array([ 5907, 11305,  8505,  8710,  1773,  7135,  6684,   777, 18398,\n",
      "       14921], dtype=int64), array([ 2939, 15359,  8490, 17626,  8505,  1368, 18232,  5907,  9201,\n",
      "         176], dtype=int64), array([17626, 18398,  5907,  1773,  1947,  6684,   777,   954,  9931,\n",
      "        8505], dtype=int64)]\n",
      "Top weights for Class 7:\n",
      "[array([18398,  5907,  4856,  7135, 18345, 11305,   777,  8710,  8490,\n",
      "        5901], dtype=int64), array([ 4856,  7135,   777,  6684, 18398,  5907,   881,  8737, 14607,\n",
      "        1773], dtype=int64), array([18398, 18232,  1368, 17626,  5907,  8505,   126,  5901,  2939,\n",
      "       15359], dtype=int64), array([ 8490, 15359,  8505,  2939, 18398, 18232,   176,  3846,   954,\n",
      "        1066], dtype=int64), array([ 5907, 11305,  8505,  8710,  1773,  7135,  6684,   777, 18398,\n",
      "       14921], dtype=int64), array([ 2939, 15359,  8490, 17626,  8505,  1368, 18232,  5907,  9201,\n",
      "         176], dtype=int64), array([17626, 18398,  5907,  1773,  1947,  6684,   777,   954,  9931,\n",
      "        8505], dtype=int64), array([ 1368,  5907, 18398, 17626,  6684,  1773,   126, 18232,  1065,\n",
      "        2939], dtype=int64)]\n",
      "Top weights for Class 8:\n",
      "[array([18398,  5907,  4856,  7135, 18345, 11305,   777,  8710,  8490,\n",
      "        5901], dtype=int64), array([ 4856,  7135,   777,  6684, 18398,  5907,   881,  8737, 14607,\n",
      "        1773], dtype=int64), array([18398, 18232,  1368, 17626,  5907,  8505,   126,  5901,  2939,\n",
      "       15359], dtype=int64), array([ 8490, 15359,  8505,  2939, 18398, 18232,   176,  3846,   954,\n",
      "        1066], dtype=int64), array([ 5907, 11305,  8505,  8710,  1773,  7135,  6684,   777, 18398,\n",
      "       14921], dtype=int64), array([ 2939, 15359,  8490, 17626,  8505,  1368, 18232,  5907,  9201,\n",
      "         176], dtype=int64), array([17626, 18398,  5907,  1773,  1947,  6684,   777,   954,  9931,\n",
      "        8505], dtype=int64), array([ 1368,  5907, 18398, 17626,  6684,  1773,   126, 18232,  1065,\n",
      "        2939], dtype=int64), array([18398,  8490,  8505,  5907, 15359,   176,  8710,   777,  9931,\n",
      "        3846], dtype=int64)]\n",
      "Top weights for Class 9:\n",
      "[array([18398,  5907,  4856,  7135, 18345, 11305,   777,  8710,  8490,\n",
      "        5901], dtype=int64), array([ 4856,  7135,   777,  6684, 18398,  5907,   881,  8737, 14607,\n",
      "        1773], dtype=int64), array([18398, 18232,  1368, 17626,  5907,  8505,   126,  5901,  2939,\n",
      "       15359], dtype=int64), array([ 8490, 15359,  8505,  2939, 18398, 18232,   176,  3846,   954,\n",
      "        1066], dtype=int64), array([ 5907, 11305,  8505,  8710,  1773,  7135,  6684,   777, 18398,\n",
      "       14921], dtype=int64), array([ 2939, 15359,  8490, 17626,  8505,  1368, 18232,  5907,  9201,\n",
      "         176], dtype=int64), array([17626, 18398,  5907,  1773,  1947,  6684,   777,   954,  9931,\n",
      "        8505], dtype=int64), array([ 1368,  5907, 18398, 17626,  6684,  1773,   126, 18232,  1065,\n",
      "        2939], dtype=int64), array([18398,  8490,  8505,  5907, 15359,   176,  8710,   777,  9931,\n",
      "        3846], dtype=int64), array([ 2939, 11305,  5907,  8678,  8505, 14921,  7135,  8710,  6684,\n",
      "        1368], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "top_weights=[]\n",
    "for idx, topic in enumerate(groupeddf['Label']): #print the top 10 words for each class\n",
    "    print(f\"Top weights for Class {topic}:\")\n",
    "    top_weights.append(ctfidf_array[idx].argsort()[-10:][::-1])\n",
    "    print(top_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes for text data\n",
    "multinomial focuses on the frequency of a word?  \n",
    "bernoulli focuses on if a word appears?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test = []\n",
    "for image, label in test_dataset: #with the for loops, took my laptop around 30 min to run on 4x4\n",
    "    r = regions(image, region_size)\n",
    "    for i in range(len(r)):\n",
    "        r[i] = encode(r[i], threshold)\n",
    "    encoded_test.append(r) #in theory this encodes all the regions and stores them. each image is its own list of decimal regions\n",
    "    \n",
    "for i in range(len(encoded_test)):\n",
    "    encoded_test[i] = [x for x in encoded_test[i] if x != 0] #remove any regions that are just 0 (all black/no important features according to threshold)\n",
    "    \n",
    "text_test = [' '.join(map(str,encoded_test[i])) for i in range(len(encoded_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_data = [' '.join(map(str,topdata[i])) for i in range(len(topdata))]\n",
    "traindata = pd.DataFrame({'Features':top_data, 'Class': labels})\n",
    "X_train = traindata['Features']\n",
    "y_train = traindata['Class']\n",
    "X_test = text_test\n",
    "y_test = test_dataset.targets.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_data_scaled = [' '.join(map(str,scaled_top_words[i])) for i in range(len(scaled_top_words))]\n",
    "traindata_scaled = pd.DataFrame({'Features':top_data_scaled, 'Class': labels})\n",
    "X_train_scaled = traindata_scaled['Features']\n",
    "y_train_scaled = traindata_scaled['Class']\n",
    "X_test = text_test\n",
    "y_test = test_dataset.targets.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "2068\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train[0]))\n",
    "print(len(X_train_scaled[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_train_vectors_scaled = vectorizer.fit_transform(X_train_scaled)\n",
    "X_test_vectors = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 22.18%\n",
      "\n",
      "Accuracy: 21.57%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model2 = BernoulliNB()\n",
    "model.fit(X_train_vectors, y_train)\n",
    "model2.fit(X_train_vectors, y_train)\n",
    "y_pred = model.predict(X_test_vectors)\n",
    "y_pred2 = model2.predict(X_test_vectors)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy2 = accuracy_score(y_test, y_pred2)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\\n\") #7x7 19-22%, 14x14 ~12-13%, 4x4 21-22% (-0.5 Binarization Threshold) (TOP 10 WORDS NO THRESHOLD)\n",
    "print(f\"Accuracy: {accuracy2 * 100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 22.18%\n",
      "\n",
      "Accuracy: 21.57%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model2 = BernoulliNB()\n",
    "model.fit(X_train_vectors_scaled, y_train)\n",
    "model2.fit(X_train_vectors_scaled, y_train)\n",
    "y_pred = model.predict(X_test_vectors)\n",
    "y_pred2 = model2.predict(X_test_vectors)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy2 = accuracy_score(y_test, y_pred2)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\\n\") #7x7 19-22%, 14x14 ~12-13%, 4x4 21-22% (-0.5 Binarization Threshold) (TOP 10 WORDS NO THRESHOLD)\n",
    "print(f\"Accuracy: {accuracy2 * 100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m filtered_images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(encoded_images):\n\u001b[1;32m---> 11\u001b[0m     filtered_image \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Convert feature name (string) back to integer\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtfidf_array\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Keep only values with TF-IDF >= 0.5\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     16\u001b[0m     filtered_images\u001b[38;5;241m.\u001b[39mappend(filtered_image)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#train_dataset = [filtered_images, train_dataset[:][[1]]]\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[107], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m filtered_images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(encoded_images):\n\u001b[0;32m     11\u001b[0m     filtered_image \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28mint\u001b[39m(feature_names[j])  \u001b[38;5;66;03m# Convert feature name (string) back to integer\u001b[39;00m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(feature_names))\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tfidf_array[i, j] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Keep only values with TF-IDF >= 0.5\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     ]\n\u001b[0;32m     16\u001b[0m     filtered_images\u001b[38;5;241m.\u001b[39mappend(filtered_image)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#train_dataset = [filtered_images, train_dataset[:][[1]]]\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# vectorizer = TfidfVectorizer() #individual document tfidf instead of class based tfidf\n",
    "# tfidf_matrix = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "# filtered_images = []\n",
    "# for i, image in enumerate(encoded_images):\n",
    "#     filtered_image = [\n",
    "#         int(feature_names[j])  # Convert feature name (string) back to integer\n",
    "#         for j in range(len(feature_names))\n",
    "#         if tfidf_array[i, j] >= 0.5  # Keep only values with TF-IDF >= 0.5\n",
    "#     ]\n",
    "#     filtered_images.append(filtered_image)\n",
    "\n",
    "# #train_dataset = [filtered_images, train_dataset[:][[1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'region_size': 6, 'threshold': -0.4, 'score_threshold': 0.009}\n",
      "Best Score: 0.7204\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to perform encoding, used by the grid search\n",
    "def process_data(region_size, threshold, score_threshold, dataset):\n",
    "    def pad_image(image, region_size):\n",
    "        height, width = image.shape[1], image.shape[2]\n",
    "        pad_height = (region_size - (height % region_size)) % region_size\n",
    "        pad_width = (region_size - (width % region_size)) % region_size\n",
    "        padded_image = torch.nn.functional.pad(image, (0, pad_width, 0, pad_height), mode='constant', value=0)\n",
    "        return padded_image\n",
    "\n",
    "    def regions(image, size):\n",
    "        padded_image = pad_image(image, size)\n",
    "        regions = []\n",
    "        for i in range(0, padded_image.shape[1], size):\n",
    "            for j in range(0, padded_image.shape[2], size):\n",
    "                region = padded_image[0, i:i+size, j:j+size]\n",
    "                regions.append(region)\n",
    "        return regions\n",
    "\n",
    "    def encode(region, threshold):\n",
    "        region = torch.where(region < threshold, 0, 1)  # Vectorized thresholding\n",
    "        binary_str = ''.join(map(str, region.flatten().int().tolist()))  # Convert to binary\n",
    "        return int(binary_str, 2)  # Convert binary to decimal\n",
    "\n",
    "    encoded_images = []\n",
    "    for image, label in dataset:\n",
    "        r = regions(image, region_size)\n",
    "        for i in range(len(r)):\n",
    "            r[i] = encode(r[i], threshold)\n",
    "        encoded_images.append(r)\n",
    "\n",
    "    # Remove empty regions (regions where no features were detected based on the threshold)\n",
    "    for i in range(len(encoded_images)):\n",
    "        encoded_images[i] = [x for x in encoded_images[i] if x != 0]  # Remove any regions that are just 0 (all black/no important features)\n",
    "\n",
    "    text_data = [' '.join(map(str, encoded_images[i])) for i in range(len(encoded_images))]\n",
    "    return text_data\n",
    "\n",
    "# Wrapper function for the model and parameter search\n",
    "def model_with_params(region_size, threshold, score_threshold, train_dataset, test_dataset):\n",
    "    # Process data for both training and testing\n",
    "    X_train = process_data(region_size, threshold, score_threshold, train_dataset)\n",
    "    X_test = process_data(region_size, threshold, score_threshold, test_dataset)\n",
    "\n",
    "    # Vectorize the data\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "    # Train the model\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train_vectors, train_dataset.targets.tolist())\n",
    "\n",
    "    # Predict and calculate accuracy\n",
    "    y_pred = model.predict(X_test_vectors)\n",
    "    accuracy = accuracy_score(test_dataset.targets.tolist(), y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# GridSearchCV wrapper for optimization\n",
    "def grid_search(train_dataset, test_dataset):\n",
    "    # Define the parameter grid for region_size, threshold, and score_threshold\n",
    "    param_grid = {\n",
    "        'region_size': [5, 6, 7], \n",
    "        'threshold': [-0.4, -0.3, -0.2, -0.1],  \n",
    "        'score_threshold': [0.009, .008, .007, 0.01, 0.02]  \n",
    "    }\n",
    "\n",
    "    # Custom evaluation function for GridSearchCV\n",
    "    def score_fn(model, X, y):\n",
    "        # Make predictions and calculate accuracy\n",
    "        y_pred = model.predict(X)\n",
    "        return accuracy_score(y, y_pred)\n",
    "\n",
    "    # Create a custom scorer function for GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=None, param_grid=param_grid, scoring=make_scorer(score_fn), cv=3, n_jobs=-1)\n",
    "\n",
    "    # Perform grid search\n",
    "    best_params = None\n",
    "    best_score = -1\n",
    "    for region_size in param_grid['region_size']:\n",
    "        for threshold in param_grid['threshold']:\n",
    "            for score_threshold in param_grid['score_threshold']:\n",
    "                # Train the model with the current parameters\n",
    "                score = model_with_params(region_size, threshold, score_threshold, train_dataset, test_dataset)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {\n",
    "                        'region_size': region_size,\n",
    "                        'threshold': threshold,\n",
    "                        'score_threshold': score_threshold\n",
    "                    }\n",
    "\n",
    "    return best_params, best_score\n",
    "\n",
    "# Example usage of the grid search\n",
    "best_params, best_score = grid_search(train_dataset, test_dataset)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Score: {best_score}\")\n",
    "#print out all runs going forward to identify the effects of parameters\n",
    "\n",
    "#~60 min to run, 71.2% accuracy, 6, -0.3, .01\n",
    "# 'region_size': [4, 5, 6, 7, 8, 9, 10?], \n",
    "# 'threshold': [-0.3, -0.5, -0.7]?,  \n",
    "# 'score_threshold': [.01, .02, .03]?  \n",
    "\n",
    "\n",
    "#~160 min to run, 72.04% accuracy, 6, -0.4, .009\n",
    "# 'region_size': [5, 6, 7], \n",
    "# 'threshold': [-0.4, -0.3, -0.2, -0.1],  \n",
    "# 'score_threshold': [0.009, .008, .007, 0.01, 0.02]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts\n",
    "Experiment with bernoullis model? maybe a different version as well  \n",
    "Also consider scaling the words for the multinomial model? may be worth a shot but based on my previous experiments with just the top 10 words it didnt really make a difference, but it may make a difference now with the ctfidf score threshold since each class will have a different amount of features within them and it wont just be exactly 10 words in each class getting scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to test\n",
    "Outputs for each individual run to see what the individual parameters do  \n",
    "Test overlappping layers  \n",
    "Test regular TFIDF  \n",
    "Test different versions of Naive Bayes  \n",
    "Test other datasets  \n",
    "Also explain ctfidf in presentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
