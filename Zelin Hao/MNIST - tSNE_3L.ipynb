{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 1 ---\n",
      "Epoch 1/16, Loss: 0.2780296324180372\n",
      "Epoch 2/16, Loss: 0.10834085456987243\n",
      "Epoch 3/16, Loss: 0.08170990516426169\n",
      "Epoch 4/16, Loss: 0.06778602217828461\n",
      "Epoch 5/16, Loss: 0.06021025189127066\n",
      "Epoch 6/16, Loss: 0.05303180185300527\n",
      "Epoch 7/16, Loss: 0.046990697711006425\n",
      "Epoch 8/16, Loss: 0.04217507906754175\n",
      "Epoch 9/16, Loss: 0.043325706797261374\n",
      "Epoch 10/16, Loss: 0.037153911329442985\n",
      "Epoch 11/16, Loss: 0.03607188630615447\n",
      "Epoch 12/16, Loss: 0.03289858349501537\n",
      "Epoch 13/16, Loss: 0.03510093424477918\n",
      "Epoch 14/16, Loss: 0.030310355722001324\n",
      "Epoch 15/16, Loss: 0.03296278324314253\n",
      "Epoch 16/16, Loss: 0.02522991570602773\n",
      "Accuracy: 97.45%\n",
      "\n",
      "--- Trial 2 ---\n",
      "Epoch 1/16, Loss: 0.3036367850485387\n",
      "Epoch 2/16, Loss: 0.11655736492891143\n",
      "Epoch 3/16, Loss: 0.08686394441420081\n",
      "Epoch 4/16, Loss: 0.07523957303010805\n",
      "Epoch 5/16, Loss: 0.06608696988779308\n",
      "Epoch 6/16, Loss: 0.058360069326603155\n",
      "Epoch 7/16, Loss: 0.05332486515076072\n",
      "Epoch 8/16, Loss: 0.04700281615991744\n",
      "Epoch 9/16, Loss: 0.04608558083837241\n",
      "Epoch 10/16, Loss: 0.044290343263407556\n",
      "Epoch 11/16, Loss: 0.03751892821091964\n",
      "Epoch 12/16, Loss: 0.037171643008136995\n",
      "Epoch 13/16, Loss: 0.038069143510432724\n",
      "Epoch 14/16, Loss: 0.03157459784758526\n",
      "Epoch 15/16, Loss: 0.035431946993050294\n",
      "Epoch 16/16, Loss: 0.03182445270988337\n",
      "Accuracy: 97.38%\n",
      "\n",
      "--- Trial 3 ---\n",
      "Epoch 1/16, Loss: 0.30545930803886484\n",
      "Epoch 2/16, Loss: 0.11532504055678829\n",
      "Epoch 3/16, Loss: 0.08292608335117764\n",
      "Epoch 4/16, Loss: 0.07240813667985248\n",
      "Epoch 5/16, Loss: 0.06365355602423373\n",
      "Epoch 6/16, Loss: 0.05572470799015253\n",
      "Epoch 7/16, Loss: 0.04979319468038584\n",
      "Epoch 8/16, Loss: 0.050269831396599136\n",
      "Epoch 9/16, Loss: 0.04017069624843764\n",
      "Epoch 10/16, Loss: 0.04313786593564937\n",
      "Epoch 11/16, Loss: 0.03958425649170264\n",
      "Epoch 12/16, Loss: 0.03348331211259998\n",
      "Epoch 13/16, Loss: 0.036256250400960845\n",
      "Epoch 14/16, Loss: 0.0347612216424467\n",
      "Epoch 15/16, Loss: 0.02906103895457694\n",
      "Epoch 16/16, Loss: 0.035073060501561955\n",
      "Accuracy: 96.98%\n",
      "\n",
      "--- Trial 4 ---\n",
      "Epoch 1/16, Loss: 0.3082677225477056\n",
      "Epoch 2/16, Loss: 0.10851993285733134\n",
      "Epoch 3/16, Loss: 0.08350842472673185\n",
      "Epoch 4/16, Loss: 0.07082131596351378\n",
      "Epoch 5/16, Loss: 0.06291876488111055\n",
      "Epoch 6/16, Loss: 0.05241156955439993\n",
      "Epoch 7/16, Loss: 0.05102613474874296\n",
      "Epoch 8/16, Loss: 0.04919928179832666\n",
      "Epoch 9/16, Loss: 0.044656417034073685\n",
      "Epoch 10/16, Loss: 0.036139596087302626\n",
      "Epoch 11/16, Loss: 0.03803245844195197\n",
      "Epoch 12/16, Loss: 0.036673217308378414\n",
      "Epoch 13/16, Loss: 0.03318966374906494\n",
      "Epoch 14/16, Loss: 0.02936822663934139\n",
      "Epoch 15/16, Loss: 0.03652649491825795\n",
      "Epoch 16/16, Loss: 0.03019995995196002\n",
      "Accuracy: 97.1%\n",
      "\n",
      "--- Trial 5 ---\n",
      "Epoch 1/16, Loss: 0.3082115137373715\n",
      "Epoch 2/16, Loss: 0.11241284822489121\n",
      "Epoch 3/16, Loss: 0.08825979243888497\n",
      "Epoch 4/16, Loss: 0.07353727418275526\n",
      "Epoch 5/16, Loss: 0.0641761978256401\n",
      "Epoch 6/16, Loss: 0.05828363044576636\n",
      "Epoch 7/16, Loss: 0.051930916927741724\n",
      "Epoch 8/16, Loss: 0.05095387657632434\n",
      "Epoch 9/16, Loss: 0.0453467570806368\n",
      "Epoch 10/16, Loss: 0.04007031106917946\n",
      "Epoch 11/16, Loss: 0.037821650823733274\n",
      "Epoch 12/16, Loss: 0.03744966428423028\n",
      "Epoch 13/16, Loss: 0.037230931585689346\n",
      "Epoch 14/16, Loss: 0.033419740536342835\n",
      "Epoch 15/16, Loss: 0.02859397421855265\n",
      "Epoch 16/16, Loss: 0.03344456745790574\n",
      "Accuracy: 97.16%\n",
      "\n",
      "Average Accuracy over 5 trials: 97.21399999999998%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97.21399999999998"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "\n",
    "# Load the MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "# Function to apply PCA and t-SNE embedding to the data\n",
    "def apply_tsne_embedding(trainset, testset):\n",
    "    # Concatenate training and testing data to apply t-SNE on the entire dataset\n",
    "    full_data = torch.cat((trainset.data, testset.data), dim=0)\n",
    "    full_labels = torch.cat((trainset.targets, testset.targets), dim=0)\n",
    "\n",
    "    # Flatten images and randomize the features\n",
    "    flattened_data = full_data.view(full_data.size(0), -1).numpy()\n",
    "    randomized_data = flattened_data[:, np.random.permutation(flattened_data.shape[1])]\n",
    "    \n",
    "    # Transpose to treat features as \"samples\" in t-SNE\n",
    "    transposed_data = randomized_data.T\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=40, metric=\"euclidean\", max_iter=300, random_state=0)\n",
    "    tsne_coordinates = tsne.fit_transform(transposed_data)\n",
    "\n",
    "    # Normalize the t-SNE coordinates to fit within a 28x28 grid\n",
    "    scaler = MinMaxScaler(feature_range=(0, 27))\n",
    "    tsne_coordinates = scaler.fit_transform(tsne_coordinates).astype(int)\n",
    "\n",
    "    # Initialize an array to hold the reorganized feature map\n",
    "    reordered_data = np.zeros((full_data.size(0), 1, 28, 28))\n",
    "    count_matrix = np.zeros((28, 28))\n",
    "\n",
    "    # Map t-SNE coordinates to the 28x28 grid for each image\n",
    "    for img_idx in range(full_data.size(0)):\n",
    "        feature_vector = transposed_data[:, img_idx]\n",
    "        grid = np.zeros((28, 28))\n",
    "        count = np.zeros((28, 28))\n",
    "\n",
    "        for feature_idx, (x, y) in enumerate(tsne_coordinates):\n",
    "            # Accumulate feature values in the grid and count occurrences\n",
    "            grid[x, y] += feature_vector[feature_idx]\n",
    "            count[x, y] += 1\n",
    "\n",
    "        # Average the values where multiple features overlap\n",
    "        grid = np.divide(grid, count, out=np.zeros_like(grid), where=count != 0)\n",
    "        reordered_data[img_idx, 0] = grid  # Place the averaged grid in the final dataset array\n",
    "\n",
    "    # Split back into training and testing sets\n",
    "    train_data = torch.tensor(reordered_data[:len(trainset)], dtype=torch.float32)\n",
    "    train_labels = trainset.targets\n",
    "    test_data = torch.tensor(reordered_data[len(trainset):], dtype=torch.float32)\n",
    "    test_labels = testset.targets\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "# Preprocess the datasets\n",
    "train_data, train_labels, test_data, test_labels = apply_tsne_embedding(trainset, testset)\n",
    "\n",
    "# Create dataloader with preprocessed data\n",
    "trainloader = torch.utils.data.DataLoader(list(zip(train_data, train_labels)), batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(list(zip(test_data, test_labels)), batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the 3-layer CNN model\n",
    "class ThreeLayerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ThreeLayerCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "def initialize_model():\n",
    "    model = ThreeLayerCNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, trainloader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(trainloader)}\")\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy}%')\n",
    "    return accuracy\n",
    "\n",
    "# Run the model multiple times and calculate the average accuracy\n",
    "def run_multiple_trials(num_trials, epochs):\n",
    "    accuracies = []\n",
    "    for trial in range(num_trials):\n",
    "        print(f\"\\n--- Trial {trial + 1} ---\")\n",
    "        model, criterion, optimizer = initialize_model()\n",
    "        train_model(model, trainloader, criterion, optimizer, epochs)\n",
    "        accuracy = evaluate_model(model, testloader)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    print(f\"\\nAverage Accuracy over {num_trials} trials: {avg_accuracy}%\")\n",
    "    return avg_accuracy\n",
    "\n",
    "# Execute the trials\n",
    "run_multiple_trials(5, 16)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
